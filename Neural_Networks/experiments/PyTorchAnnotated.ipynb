{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break Down of running a network on CIFAR10\n",
    "Hi all, this is a notebook to help break down running a network on CIFAR10. It will be similar to the tutorial on the PyTorch website; however, I'll go over some pointers for making some deep learning code more modular, specfic issues you may run into with your assignment, and a few extra things for those who are intrested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "torch - PyTorch module \n",
    "\n",
    "torchvision - PyTorch Vision library\n",
    "\n",
    "torch.optim - PyTorch optimizer module, you implimented SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the CIFAR10 dataset\n",
    "PyTorch vision makes it really easy to import some of the well known datasets here's more information on some of them: https://pytorch.org/docs/stable/torchvision/datasets.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data transformation to normalize data\n",
    "# normalizing data helps your optimization algorithm converge more quickly\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# training set split\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                        train=True,\n",
    "                                        download=True, \n",
    "                                        transform=transform)\n",
    "\n",
    "# testing set split, this portion is similar to your assignment\n",
    "# but instead of passing in trainset you need to pass in an instance \n",
    "# of a custom data object which is provided in the starter code\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                       train=False,\n",
    "                                       download=True, \n",
    "                                       transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, \n",
    "                                         batch_size=10,\n",
    "                                         shuffle=False, \n",
    "                                         num_workers=2)\n",
    "\n",
    "# labeling the classes of images just like the digits of MNIST, or the \n",
    "# the breeds of dogs from DogNet\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the GPU\n",
    "\n",
    "One of the most best ways to speed up the training process of a deep network is to use a GPU (or TPU if you can). This is because with a GPU we can train a whole batch of data in parallel. This section is optinal and i'll mark the other parts of the code where this comes up. But, for those of you intrested and able to utilze a GPU I highly reccomend it! To do so just install CUDA and update your GPU drivers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available()  else 'cpu')\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the images\n",
    "\n",
    "This function just plots some of the imagges from the training set, I would pay attention to this section because it can help make plotting the images from dogset and synth data a lot easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img/2  +0.5  # have to unnormalize the images\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your network\n",
    "Below we build a simple network and the forward pass of the network. On the PyTorch website they use some notation which is slightly dated.\n",
    "\n",
    "`import torch.nn.functional as F\n",
    "F.relu(...)`\n",
    "\n",
    "Most activation functions are just in the torch.nn module now. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Functions\n",
    "Here I've just made the training and testing portions of the orignal tutorial into functions. I added some more comments to further break down what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, trainloader, epochs=10, devloader=None, print_info=True):\n",
    "    epochs_loss = []\n",
    "    epoch_acc = []\n",
    "    running_loss = 0.0\n",
    "    # the length of the train loader will give you the number of mini-batches\n",
    "    # not the cleanest solution but for now it generalizes well and avoids \n",
    "    # computation that we don't need\n",
    "    minibatches = len(trainloader)\n",
    "    # moving the network onto the gpu/cpu\n",
    "    model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch, labels in trainloader:\n",
    "            # batch is a tensor with m elements, where each element is \n",
    "            # a training example\n",
    "            \n",
    "            # moving batch/labels onto the gpu/cpu\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            \n",
    "            # zeroing the parameters of the model \n",
    "            # becuase we want to optimize them\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            # getting the predictions from our model by passing in a mini-batch\n",
    "            # the ouput will have shape (mini-batch-size, number-of-classes)\n",
    "            # where each element of output is the probabliity of that example being\n",
    "            # the classification correspoding to the index of the value\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # optimize the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # add the loss of a mini-batch to the list of epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        #  after each epoch we need to average out the loss across all minibatches\n",
    "        epochs_loss.append(epoch_loss/minibatches)\n",
    "        # printing some info\n",
    "        if print_info:\n",
    "            print(f'Epoch: {epoch} Loss: {epoch_loss/minibatches}')\n",
    "    return model, epoch_loss\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, testloader):\n",
    "    # variables to keep count of correct labels and the total labels in a mini batch\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're testing the model we don't need to perform backprop\n",
    "    with torch.no_grad():\n",
    "        for batch, labels in testloader:\n",
    "            batch, labels = batch, labels\n",
    "            output = model(batch)\n",
    "            # this gives us the index with the highest value outputed from the last layer\n",
    "            # which coressponds to the most probable label/classification for an image\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Putting it together \n",
    "Here we make a model from the Net() and train it for 100 epochs, and then test it on our testing data we set aside earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net, loss = train_model(net,\n",
    "                        epochs=1,\n",
    "                        criterion = nn.CrossEntropyLoss(),\n",
    "                        optimizer = optim.Adam(net.parameters(), lr=0.01, betas=(0.9, 0.999)),\n",
    "                        trainloader=trainloader\n",
    "                        )\n",
    "net.to('cpu')\n",
    "test_model(net, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading a model\n",
    "When working with a network it can take a long time to train. To save sometime if you have to come back to a project you can save the weights of a model using the cell below. The cell following just shows how you can load the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a model\n",
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
